{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google_play_scraper as gps\n",
    "from urllib.request import urlretrieve, urlopen, Request\n",
    "from urllib.parse import quote\n",
    "from urllib.parse import urlencode\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##nohup wget -r -p -e robots=off -U mozilla play.google.com/store/apps &\n",
    "### This takes a while and gives inconsistent results\n",
    "import os\n",
    "import pandas as pd\n",
    "path = \"play.google.com/store/apps/\"\n",
    "devs = os.listdir(path)\n",
    "ids = [path+dev for dev in devs]\n",
    "df = pd.DataFrame({\"ids\":ids})\n",
    "df.to_csv(\"devs.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class GoogleSpider(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"Crawl Google search results\n",
    "\n",
    "        This class is used to crawl Google's search results using requests and BeautifulSoup.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:79.0) Gecko/20100101 Firefox/79.0',\n",
    "            'Host': 'www.google.com',\n",
    "            'Referer': 'https://www.google.com/'\n",
    "        }\n",
    "        self.devs = None\n",
    "        self.apps = None\n",
    "\n",
    "    def __get_source(self, url: str):\n",
    "        \"\"\"Get the web page's source code\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to crawl\n",
    "\n",
    "        Returns:\n",
    "            requests.Response: The response from URL\n",
    "        \"\"\"\n",
    "        \n",
    "        req = Request(url)\n",
    "        req.add_header('User-Agent','Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:79.0) Gecko/20100101 Firefox/79.0')\n",
    "        req.add_header('Host', 'www.google.com')\n",
    "        req.add_header('Referer', 'https://www.google.com/')\n",
    "        return urlopen(req)\n",
    "\n",
    "    def search_devs(self, query: str) -> list:\n",
    "        \"\"\"Search Google\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search for\n",
    "\n",
    "        Returns:\n",
    "            list: The search results\n",
    "        \"\"\"\n",
    "        numPages = 0\n",
    "        \n",
    "        results = {}\n",
    "        container_next = None\n",
    "        _next = None\n",
    "        while numPages < 35:\n",
    "            #print(\"results\")\n",
    "            #print(len(results))\n",
    "            numPages+=1\n",
    "            # Get response\n",
    "            if numPages ==1:\n",
    "                response = self.__get_source('https://www.google.com/search?q=%s' % quote(query))\n",
    "                #print(\"first url\")\n",
    "                #print('https://www.google.com/search?q=%s' % quote(query))\n",
    "            else:\n",
    "                \n",
    "                next_url = 'https://www.google.com' +_next\n",
    "                response = self.__get_source(next_url)\n",
    "                #print(\"next url\")\n",
    "                #print(next_url)\n",
    "           # stop = input()\n",
    "            #if stop == \"q\":\n",
    "             #   return\n",
    "            #print(response)\n",
    "            # Initialize BeautifulSoup\n",
    "            soup = BeautifulSoup(response.read(), 'html.parser')\n",
    "            if numPages < 35:\n",
    "                cont= soup.find_all('div', role='navigation')[-1]\n",
    "                next_pages = cont.find_all('a', class_= \"fl\")\n",
    "                page_string = 'Page ' + str(numPages + 1)\n",
    "                \n",
    "                for page in next_pages:\n",
    "                    if page.get('aria-label') == page_string:\n",
    "                        _next = page.get(\"href\")\n",
    "                        #print(\"Found _next page\")\n",
    "                        #print(page_string)\n",
    "                        #print(_next)\n",
    "                        break\n",
    "\n",
    "            # Get the result containers\n",
    "            result_containers = soup.findAll('div', class_='yuRUbf')\n",
    "            # Final results list\n",
    "\n",
    "            # Loop through every container\n",
    "            for container in result_containers:\n",
    "                #print(container)\n",
    "                # Result title\n",
    "                title = container.find('h3').text\n",
    "              # Result URL\n",
    "                url = container.find('a')['href']\n",
    "                # Result description\n",
    "                if title in results:\n",
    "                    print(\"dev already seen\")\n",
    "                    print(title)\n",
    "                else:\n",
    "                    results[title] = url\n",
    "\n",
    "\n",
    "\n",
    "        self.devs = results\n",
    "        return self.devs\n",
    "    \n",
    "    def get_app_id(self):\n",
    "        \n",
    "        if not self.devs:\n",
    "            self.devs = pd.read_csv(\"devIds.csv\", sep = \",\")\n",
    "        apps = []\n",
    "        failed = []\n",
    "        i = 0\n",
    "        for url in self.devs[\"ids\"]:\n",
    "            i+=1\n",
    "            try:\n",
    "\n",
    "\n",
    "                response = urlopen(url)\n",
    "                soup = BeautifulSoup(str(response.read()), 'html.parser')\n",
    "                result = soup.find_all('div', class_=\"ImZGtf mpg5gc\")\n",
    "                for res in result:\n",
    "                    ref = res.find('a', class_ = 'poRVub')\n",
    "                    part = str(ref).split(\"details?id=\")[1]\n",
    "                    apps.append(part.split('\"')[0])\n",
    "            except:\n",
    "                failed.append(url)\n",
    "                \n",
    "        self.apps = apps\n",
    "        self.failed = failed\n",
    "        return apps, failed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'site:\\\"https://play.google.com/store/apps/developer?id=\\\"'\n",
    "google = GoogleSpider()\n",
    "devs = google.search_devs(query)\n",
    "##Too few devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##nohup wget -r -p -e robots=off -U mozilla play.google.com/store/apps &\n",
    "### This takes a while and gives inconsistent results\n",
    "import os\n",
    "import pandas as pd\n",
    "path = \"play.google.com/store/apps/\"\n",
    "devs = os.listdir(path)\n",
    "ids = [path+dev for dev in devs]\n",
    "df = pd.DataFrame({\"ids\":ids})\n",
    "df.to_csv(\"devIds.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = GoogleSpider()\n",
    "apps = google.get_app_id()\n",
    "df = pd.DataFrame({\"App Id\": apps})\n",
    "df.to_csv(\"apps.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
